{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/audy/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/home/audy/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "#%% Necessary packages\n",
    "# 1. Keras\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.optimizers import *\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from keras import regularizers\n",
    "from keras import backend as K\n",
    "K.set_image_data_format('channels_first')\n",
    "# 2. Others\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# 3. PyTorch\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms, utils\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import io, transform, color\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "#ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset(Dataset):\n",
    "\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "\n",
    "        self.data_frame = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.mask_dir = self.root_dir.replace('CBIS-DDSM_classification','masks')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.root_dir,self.data_frame.iloc[idx]['name'])\n",
    "        image = Image.open(img_name)\n",
    "\n",
    "        label = self.data_frame.iloc[idx]['category']\n",
    "\n",
    "        mask_name = os.path.join(self.mask_dir,self.data_frame.iloc[idx]['name'].replace('.j','_mask.j'))\n",
    "        mask = io.imread(mask_name)\n",
    "        mask = np.array([mask,mask,mask]).transpose((1,2,0))\n",
    "        mask = Image.fromarray(mask)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            mask = self.transform(mask) \n",
    "      \n",
    "        return {'image':image,'category':label,'mask':mask, 'name':img_name}\n",
    "    \n",
    "\n",
    "def get_dataloader(data_dir, train_csv_path, image_size, img_mean, img_std, batch_size=1):\n",
    "\n",
    "    data_transforms = {\n",
    "        'train': transforms.Compose([\n",
    "            transforms.Resize(image_size),#row to column ratio should be 1.69\n",
    "            #transforms.RandomHorizontalFlip(0.5),\n",
    "            transforms.RandomVerticalFlip(0.5),\n",
    "            transforms.RandomRotation(30),\n",
    "            transforms.ToTensor(),\n",
    "            #transforms.Normalize([0.223, 0.231, 0.243], [0.266, 0.270, 0.274])\n",
    "            transforms.Normalize(img_mean,img_std)\n",
    "        ]),\n",
    "        'valid': transforms.Compose([\n",
    "            transforms.Resize(image_size),\n",
    "            transforms.ToTensor(),\n",
    "            #transforms.Normalize([0.223, 0.231, 0.243], [0.266, 0.270, 0.274])\n",
    "            transforms.Normalize(img_mean,img_std)\n",
    "        ]),\n",
    "        'test': transforms.Compose([\n",
    "            transforms.Resize(image_size),\n",
    "            transforms.ToTensor(),\n",
    "            #transforms.Normalize([0.223, 0.231, 0.243], [0.266, 0.270, 0.274])\n",
    "            transforms.Normalize(img_mean,img_std)\n",
    "        ])\n",
    "    }\n",
    "\n",
    "    image_datasets = {}\n",
    "    dataloaders = {}\n",
    "    dataset_sizes = {}\n",
    "\n",
    "    for x in ['train', 'valid', 'test']:\n",
    "        image_datasets[x] = dataset(train_csv_path.replace('train',x),root_dir=data_dir,transform=data_transforms[x])\n",
    "\n",
    "        if x!= 'test':\n",
    "            dataloaders[x] = torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size,shuffle=True, num_workers=4)\n",
    "        else:\n",
    "            dataloaders[x] = torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size,shuffle=False, num_workers=4)\n",
    "        dataset_sizes[x] = len(image_datasets[x])\n",
    "\n",
    "    device = torch.device(\"cuda:0\")\n",
    "\n",
    "    return dataloaders,dataset_sizes,image_datasets,device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../Data/CBIS-DDSM_classification_orient/'\n",
    "train_csv = '../CSV/gain_train.csv'\n",
    "#image_size = (640,384)\n",
    "image_size = (320,192)\n",
    "batch_size = 12\n",
    "img_mean = [0.223, 0.231, 0.243]\n",
    "img_std = [0.266, 0.270, 0.274]\n",
    "\n",
    "dataloaders,dataset_sizes,dataset,device = get_dataloader(data_dir,train_csv,image_size,img_mean,img_std,batch_size)\n",
    "\n",
    "\n",
    "a = iter(dataloaders['train']).next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Define PVS class\n",
    "class PVS():\n",
    "    \n",
    "    # 1. Initialization\n",
    "    '''\n",
    "    x_train: training samples\n",
    "    data_type: Syn1 to Syn 6\n",
    "    '''\n",
    "    def __init__(self, x_train, data_type, lamda):\n",
    "        \n",
    "        self.data_dir = '../Data/CBIS-DDSM_classification_orient/'\n",
    "        self.train_csv = '../CSV/gain_train.csv'\n",
    "        self.input_shape = (320,192)\n",
    "        self.batch_size = 12\n",
    "        self.img_mean = [0.223, 0.231, 0.243]\n",
    "        self.img_std = [0.266, 0.270, 0.274]\n",
    "        self.alpha = \n",
    "\n",
    "        # Use Adam optimizer with learning rate = 0.0001\n",
    "        optimizer = Adam(0.0001)\n",
    "        \n",
    "        # Build and compile the predictor\n",
    "        self.predictor = self.build_predictor_discriminator('predictor')\n",
    "        # Use categorical cross entropy as the loss\n",
    "        self.predictor.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['acc'])\n",
    "\n",
    "        # Build and compile the predictor\n",
    "        self.discriminator = self.build_predictor_discriminator('discriminator')\n",
    "        # Use categorical cross entropy as the loss\n",
    "        self.discriminator.compile(loss=self.my_nce_loss(), optimizer=optimizer, metrics=['acc'])\n",
    "\n",
    "        # Build the generator (actor)\n",
    "        self.selector = self.build_selector()\n",
    "        # Use custom loss (my loss)\n",
    "        self.selector.compile(loss=self.my_loss, optimizer=optimizer)\n",
    "\n",
    "    def my_nce_loss(y_true, y_pred):\n",
    "        return -K.categorical_crossentropy(y_true, y_pred)\n",
    "        \n",
    "    #%% Custom loss definition\n",
    "    def my_loss(self, y_true, y_pred):\n",
    "        \n",
    "        # dimension of the features\n",
    "        d = y_true.shape[1]//3        \n",
    "        \n",
    "        # Put all three in y_true \n",
    "        # 1. predictor probability\n",
    "        p_prob = y_true[:,:d]\n",
    "        # 2. discriminator output\n",
    "        d_prob = y_true[:,d:2*d]\n",
    "        # 3. ground truth\n",
    "        y_final = y_true[:,2*d:]\n",
    "        \n",
    "        ce_pred = tf.reduce_sum(K.categorical_crossentropy(y_final, p_prob, axis=1))\n",
    "        ce_dis = tf.reduce_sum(K.categorical_crossentropy(y_final, d_prob, axis=1))\n",
    "        l2_norm = tf.reduce_sum(tf.norm(y_pred, ord='euclidean', axis=[1,2,3]))\n",
    "\n",
    "        loss = tf.reduce_mean(self.alpha*ce_pred - ce_dis + self.beta*l2_norm)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    #%% Selector\n",
    "    def build_selector():\n",
    "\n",
    "        inputs = Input((256,256,1))\n",
    "        conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(inputs)\n",
    "        conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv1)\n",
    "        pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "        conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool1)\n",
    "        conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv2)\n",
    "        pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "        conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool2)\n",
    "        conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv3)\n",
    "        pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "        conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool3)\n",
    "        conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv4)\n",
    "        drop4 = Dropout(0.5)(conv4)\n",
    "        pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)\n",
    "\n",
    "        conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool4)\n",
    "        conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv5)\n",
    "        drop5 = Dropout(0.5)(conv5)\n",
    "\n",
    "        up6 = Conv2D(512, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(drop5))\n",
    "        #merge6 = merge([drop4,up6], mode = 'concat', concat_axis = 3)\n",
    "        merge6 = Concatenate()([drop4,up6])#, axis=-1)\n",
    "        conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge6)\n",
    "        conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv6)\n",
    "\n",
    "        up7 = Conv2D(256, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv6))\n",
    "        #merge7 = merge([conv3,up7], mode = 'concat', concat_axis = 3)\n",
    "        merge7 = Concatenate()([conv3,up7])#, axis=-1)\n",
    "        conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge7)\n",
    "        conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv7)\n",
    "\n",
    "        up8 = Conv2D(128, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv7))\n",
    "        #merge8 = merge([conv2,up8], mode = 'concat', concat_axis = 3)\n",
    "        merge8 = Concatenate()([conv2,up8])#, axis=-1)\n",
    "        conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge8)\n",
    "        conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv8)\n",
    "\n",
    "        up9 = Conv2D(64, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv8))\n",
    "        #merge9 = merge([conv1,up9], mode = 'concat', concat_axis = 3)\n",
    "        merge9 = Concatenate()([conv1,up9])#, axis=-1)\n",
    "        conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge9)\n",
    "        conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
    "        conv9 = Conv2D(2, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
    "        conv10 = Conv2D(1, 1, activation = 'sigmoid')(conv9)\n",
    "\n",
    "        model = Model(inputs = inputs, outputs = conv10)\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        return model\n",
    "\n",
    "\n",
    "    #%% Discriminator (Critic)\n",
    "    #We will share weights between the predictor and the discriminator. \n",
    "    #mode = predictor => select = select\n",
    "    #mode = discriminator => select = 1 - select\n",
    "    \n",
    "    def build_predictor_discriminator(self,mode):\n",
    "        \n",
    "        # create the base pre-trained model\n",
    "        base_model = InceptionV3(weights='imagenet', include_top=False)\n",
    "\n",
    "        # add a global spatial average pooling layer\n",
    "        x = base_model.output\n",
    "        x = GlobalAveragePooling2D()(x)\n",
    "        # let's add a fully-connected layer\n",
    "        x = Dense(1024, activation='relu')(x)\n",
    "        # and a logistic layer -- we have 2 classes\n",
    "        predictions = Dense(2, activation='softmax')(x)\n",
    "\n",
    "        model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "        model.summary()\n",
    "        \n",
    "        # There are two inputs to be used in the discriminator\n",
    "        # 1. Features\n",
    "        feature = Input(shape=(self.input_shape,), dtype='float32')\n",
    "        # 2. Selected Features\n",
    "        select = Input(shape=(self.input_shape,), dtype='float32')         \n",
    "        \n",
    "        # Element-wise multiplication\n",
    "        if mode == 'discriminator':\n",
    "            select = tf.subtract(tf.ones(tf.TensorShape(select)),select)\n",
    "            \n",
    "        model_input = Multiply()([feature, select])\n",
    "        prob = model(model_input)\n",
    "\n",
    "        return Model([feature, select], prob)\n",
    "\n",
    "    #%% Sampling the features based on the output of the generator\n",
    "    def Sample_M(self, gen_prob):\n",
    "        \n",
    "        # Shape of the selection probability\n",
    "        n = gen_prob.shape[0]\n",
    "        h = gen_prob.shape[1]\n",
    "        w = gen_prob.shape[2]\n",
    "            \n",
    "        # Sampling\n",
    "        samples = np.random.binomial(1, gen_prob, (n,d,w))\n",
    "        \n",
    "        return samples\n",
    "\n",
    "    #%% Training procedure\n",
    "    def train(self, x_train, y_train):\n",
    "        \n",
    "        data_dir = '../Data/CBIS-DDSM_classification_orient/'\n",
    "        train_csv = '../CSV/gain_train.csv'\n",
    "        #image_size = (640,384)\n",
    "        image_size = (320,192)\n",
    "        batch_size = 12\n",
    "        img_mean = [0.223, 0.231, 0.243]\n",
    "        img_std = [0.266, 0.270, 0.274]\n",
    "\n",
    "        dataloaders,dataset_sizes,dataset,device = get_dataloader(data_dir,train_csv,image_size,img_mean,img_std,batch_size)\n",
    "\n",
    "\n",
    "        # For each epoch (actually iterations)\n",
    "        for epoch in range(self.epochs):\n",
    "\n",
    "            \n",
    "            #%% Train Discriminator\n",
    "            # Select a random batch of samples\n",
    "            idx = np.random.randint(0, x_train.shape[0], self.batch_size)\n",
    "            x_batch = x_train[idx,:]\n",
    "            y_batch = y_train[idx,:]\n",
    "\n",
    "            # Generate a batch of probabilities of feature selection\n",
    "            sel_prob = self.generator.predict(x_batch)\n",
    "            \n",
    "            # Sampling the features based on the generated probability\n",
    "            #sel_prob = self.Sample_M(gen_prob)     \n",
    "            \n",
    "            # Compute the prediction of the critic based on the sampled features (used for generator training)\n",
    "            pred_prob = self.predictor.predict([x_batch, sel_prob])\n",
    "            \n",
    "            dis_prob = self.discriminator.predict([x_batch, sel_prob])\n",
    "\n",
    "            # Train the predictor\n",
    "            p_loss = self.predictor.train_on_batch([x_batch, sel_prob], y_batch)\n",
    "            \n",
    "            # Train the discriminator\n",
    "            d_loss = self.discriminator.train_on_batch([x_batch, sel_prob], y_batch)\n",
    "\n",
    "            #%% Train selector\n",
    "            # Use three things as the y_true: sel_prob, dis_prob, and ground truth (y_batch)\n",
    "            y_batch_final = np.concatenate( (np.asarray(p_prob), np.asarray(dis_prob), y_batch), axis = 1 )\n",
    "            #y_batch_final = [np.asarray(pred_prob),np.asarray(dis_prob),y_batch]\n",
    "            \n",
    "            # Train the generator\n",
    "            g_loss = self.selector.train_on_batch(x_batch, y_batch_final)\n",
    "\n",
    "            #%% Plot the progress\n",
    "            dialog = 'Epoch: ' + str(epoch) + ', p_loss (CE): ' + str(np.round(p_loss[0],4)) + ', p_loss (Acc): ' + str(p_loss[1]) + ', g_loss: ' + str(np.round(g_loss,4))\n",
    " \n",
    "            if epoch % 100 == 0:              \n",
    "                print(dialog)\n",
    "    \n",
    "    #%% Selected Features        \n",
    "    def output(self, x_train):\n",
    "        \n",
    "        gen_prob = self.selector.predict(x_train)\n",
    "        \n",
    "        return np.asarray(gen_prob)\n",
    "\n",
    "\n",
    "#%% Main Function\n",
    "if __name__ == '__main__':\n",
    "        \n",
    "    # Data generation function import\n",
    "    from Data_Generation import generate_data\n",
    "    \n",
    "    #%% Parameters\n",
    "    # Synthetic data type    \n",
    "    idx = 5\n",
    "    data_sets = ['Syn1','Syn2','Syn3','Syn4','Syn5','Syn6']\n",
    "    data_type = data_sets[idx]\n",
    "    \n",
    "    # No need to provide the number of relevant features at all!\n",
    "\n",
    "    # Data output can be either binary (Y) or Probability (Prob)\n",
    "    data_out_sets = ['Y','Prob']\n",
    "    data_out = data_out_sets[0]\n",
    "    \n",
    "    # Number of Training and Testing samples\n",
    "    train_N = 10000\n",
    "    test_N = 10000\n",
    "    \n",
    "    # Seeds (different seeds for training and testing)\n",
    "    train_seed = 0\n",
    "    test_seed = 1\n",
    "        \n",
    "    #%% Data Generation (Train/Test)\n",
    "    def create_data(data_type, data_out): \n",
    "        \n",
    "        x_train, y_train, g_train = generate_data(n = train_N, data_type = data_type, seed = train_seed, out = data_out)  \n",
    "        x_test,  y_test,  g_test  = generate_data(n = test_N,  data_type = data_type, seed = test_seed,  out = data_out)  \n",
    "    \n",
    "        return x_train, y_train, g_train, x_test, y_test, g_test\n",
    "    \n",
    "    x_train, y_train, g_train, x_test, y_test, g_test = create_data(data_type, data_out)\n",
    "\n",
    "    #%% Hyperparameter\n",
    "    lamda = 3\n",
    "\n",
    "    # 1. PVS Class call\n",
    "    PVS_Alg = PVS(x_train, data_type, lamda)\n",
    "        \n",
    "    # 2. Algorithm training\n",
    "    PVS_Alg.train(x_train, y_train)    \n",
    "    \n",
    "    # 3. Get the selection probability on the testing set\n",
    "    Sel_Prob_Test = PVS_Alg.output(x_test)\n",
    "    \n",
    "    # 4. Selected features\n",
    "    score = 1.*(Sel_Prob_Test > 0.5)\n",
    "    \n",
    "    #%% Performance Metrics\n",
    "    def performance_metric(score, g_truth):\n",
    "\n",
    "        n = len(score)\n",
    "        Temp_TPR = np.zeros([n,])\n",
    "        Temp_FDR = np.zeros([n,])\n",
    "        \n",
    "        for i in range(n):\n",
    "    \n",
    "            # TPR    \n",
    "            TPR_Nom = np.sum(score[i,:] * g_truth[i,:])\n",
    "            TPR_Den = np.sum(g_truth[i,:])\n",
    "            Temp_TPR[i] = 100 * float(TPR_Nom)/float(TPR_Den+1e-8)\n",
    "        \n",
    "            # FDR\n",
    "            FDR_Nom = np.sum(score[i,:] * (1-g_truth[i,:]))\n",
    "            FDR_Den = np.sum(score[i,:])\n",
    "            Temp_FDR[i] = 100 * float(FDR_Nom)/float(FDR_Den+1e-8)\n",
    "    \n",
    "        return np.mean(Temp_TPR), np.mean(Temp_FDR), np.std(Temp_TPR), np.std(Temp_FDR)\n",
    "    \n",
    "    #%% Output\n",
    "        \n",
    "    TPR_mean, FDR_mean, TPR_std, FDR_std = performance_metric(score, g_test)\n",
    "        \n",
    "    print('TPR mean: ' + str(np.round(TPR_mean,1)) + '\\%, ' + 'TPR std: ' + str(np.round(TPR_std,1)) + '\\%, '  )\n",
    "    print('FDR mean: ' + str(np.round(FDR_mean,1)) + '\\%, ' + 'FDR std: ' + str(np.round(FDR_std,1)) + '\\%, '  )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
