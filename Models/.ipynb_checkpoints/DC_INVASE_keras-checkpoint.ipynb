{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Necessary packages\n",
    "# 1. Keras\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.optimizers import *\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from keras import regularizers\n",
    "from keras import backend as K\n",
    "\n",
    "\n",
    "# 2. Others\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "#ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_generator():\n",
    "\n",
    "    inputs = Input((256,256,1))\n",
    "    conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(inputs)\n",
    "    conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "    conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool1)\n",
    "    conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "    conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool2)\n",
    "    conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv3)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "    conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool3)\n",
    "    conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv4)\n",
    "    drop4 = Dropout(0.5)(conv4)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)\n",
    "\n",
    "    conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool4)\n",
    "    conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv5)\n",
    "    drop5 = Dropout(0.5)(conv5)\n",
    "\n",
    "    up6 = Conv2D(512, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(drop5))\n",
    "    #merge6 = merge([drop4,up6], mode = 'concat', concat_axis = 3)\n",
    "    merge6 = Concatenate([drop4,up6])#, axis=-1)\n",
    "    conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge6)\n",
    "    conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv6)\n",
    "\n",
    "    up7 = Conv2D(256, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv6))\n",
    "    #merge7 = merge([conv3,up7], mode = 'concat', concat_axis = 3)\n",
    "    merge7 = Concatenate([conv3,up7])#, axis=-1)\n",
    "    conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge7)\n",
    "    conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv7)\n",
    "\n",
    "    up8 = Conv2D(128, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv7))\n",
    "    #merge8 = merge([conv2,up8], mode = 'concat', concat_axis = 3)\n",
    "    merge8 = Concatenate([conv2,up8])#, axis=-1)\n",
    "    conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge8)\n",
    "    conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv8)\n",
    "\n",
    "    up9 = Conv2D(64, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv8))\n",
    "    #merge9 = merge([conv1,up9], mode = 'concat', concat_axis = 3)\n",
    "    merge9 = Concatenate([conv1,up9])#, axis=-1)\n",
    "    conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge9)\n",
    "    conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
    "    conv9 = Conv2D(2, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
    "    conv10 = Conv2D(1, 1, activation = 'sigmoid')(conv9)\n",
    "\n",
    "    model = Model(input = inputs, output = conv10)\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "#         feature = Input(shape=(self.input_shape,), dtype='float32')\n",
    "#         select_prob = model(feature)\n",
    "\n",
    "    #return Model(feature, select_prob)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Layer conv2d_35 was called with an input that isn't a symbolic tensor. Received type: <class 'keras.layers.merge.Concatenate'>. Full input: [<keras.layers.merge.Concatenate object at 0x7fbd09ed31d0>]. All inputs to the layer should be tensors.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    278\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m                 \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_keras_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mis_keras_tensor\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    473\u001b[0m         raise ValueError('Unexpectedly found an instance of type `' +\n\u001b[0;32m--> 474\u001b[0;31m                          \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'`. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    475\u001b[0m                          'Expected a symbolic tensor instance.')\n",
      "\u001b[0;31mValueError\u001b[0m: Unexpectedly found an instance of type `<class 'keras.layers.merge.Concatenate'>`. Expected a symbolic tensor instance.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-15abf2fe418f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0munet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-14-2a59a9b2fefb>\u001b[0m in \u001b[0;36mbuild_generator\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m#merge6 = merge([drop4,up6], mode = 'concat', concat_axis = 3)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mmerge6\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdrop4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mup6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#, axis=-1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mconv6\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConv2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'relu'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'same'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_initializer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'he_normal'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmerge6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0mconv6\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConv2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'relu'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'same'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_initializer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'he_normal'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    412\u001b[0m                 \u001b[0;31m# Raise exceptions in case the input is not compatible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m                 \u001b[0;31m# with the input_spec specified in the layer constructor.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 414\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_input_compatibility\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    415\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m                 \u001b[0;31m# Collect input shapes to build layer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    283\u001b[0m                                  \u001b[0;34m'Received type: '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m                                  \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'. Full input: '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m                                  \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'. All inputs to the layer '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m                                  'should be tensors.')\n\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Layer conv2d_35 was called with an input that isn't a symbolic tensor. Received type: <class 'keras.layers.merge.Concatenate'>. Full input: [<keras.layers.merge.Concatenate object at 0x7fbd09ed31d0>]. All inputs to the layer should be tensors."
     ]
    }
   ],
   "source": [
    "unet = build_generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Define PVS class\n",
    "class PVS():\n",
    "    \n",
    "    # 1. Initialization\n",
    "    '''\n",
    "    x_train: training samples\n",
    "    data_type: Syn1 to Syn 6\n",
    "    '''\n",
    "    def __init__(self, x_train, data_type, lamda):\n",
    "        self.latent_dim1 = 100      # Dimension of actor (generator) network\n",
    "        self.latent_dim2 = 200      # Dimension of critic (discriminator) network\n",
    "        \n",
    "        self.batch_size = 1000      # Batch size\n",
    "        self.epochs = 10000         # Epoch size (large epoch is needed due to the policy gradient framework)\n",
    "        self.lamda = lamda            # Hyper-parameter for the number of selected features\n",
    "\n",
    "        self.input_shape = x_train.shape[1]     # Input dimension\n",
    "        \n",
    "        # Actionvation. (For Syn1 and 2, relu, others, selu)\n",
    "        self.activation = 'relu' if data_type in ['Syn1','Syn2'] else 'selu'       \n",
    "\n",
    "        # Use Adam optimizer with learning rate = 0.0001\n",
    "        optimizer = Adam(0.0001)\n",
    "        \n",
    "        # Build and compile the discriminator (critic)\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        # Use categorical cross entropy as the loss\n",
    "        self.discriminator.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['acc'])\n",
    "\n",
    "        # Build the generator (actor)\n",
    "        self.generator = self.build_generator()\n",
    "        # Use custom loss (my loss)\n",
    "        self.generator.compile(loss=self.my_loss, optimizer=optimizer)\n",
    "\n",
    "    #%% Custom loss definition\n",
    "    def my_loss(self, y_true, y_pred):\n",
    "        \n",
    "        # dimension of the features\n",
    "        d = y_pred.shape[1]        \n",
    "        \n",
    "        # Put all three in y_true \n",
    "        # 1. selected probability\n",
    "        sel_prob = y_true[:,:d]\n",
    "        # 2. discriminator output\n",
    "        dis_prob = y_true[:,d:(d+2)]\n",
    "        # 3. ground truth\n",
    "        y_final = y_true[:,(d+2):]        \n",
    "        \n",
    "        # A. Compute the rewards of the actor network\n",
    "        Reward = tf.reduce_sum(y_final * tf.log(dis_prob + 1e-8), axis = 1)  \n",
    "\n",
    "        # B. Policy gradient loss computation. \n",
    "        loss1 = Reward * tf.reduce_sum( sel_prob * K.log(y_pred + 1e-8) + (1-sel_prob) * K.log(1-y_pred + 1e-8), axis = 1) - self.lamda * tf.reduce_mean(y_pred, axis = 1)\n",
    "        \n",
    "        # C. Maximize the loss1\n",
    "        loss = tf.reduce_mean(-loss1)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    #%% Generator (Actor)\n",
    "    def build_generator(self):\n",
    "        \n",
    "        conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(inputs)\n",
    "        conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv1)\n",
    "        pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "        conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool1)\n",
    "        conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv2)\n",
    "        pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "        conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool2)\n",
    "        conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv3)\n",
    "        pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "        conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool3)\n",
    "        conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv4)\n",
    "        drop4 = Dropout(0.5)(conv4)\n",
    "        pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)\n",
    "\n",
    "        conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool4)\n",
    "        conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv5)\n",
    "        drop5 = Dropout(0.5)(conv5)\n",
    "\n",
    "        up6 = Conv2D(512, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(drop5))\n",
    "        merge6 = merge([drop4,up6], mode = 'concat', concat_axis = 3)\n",
    "        conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge6)\n",
    "        conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv6)\n",
    "\n",
    "        up7 = Conv2D(256, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv6))\n",
    "        merge7 = merge([conv3,up7], mode = 'concat', concat_axis = 3)\n",
    "        conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge7)\n",
    "        conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv7)\n",
    "\n",
    "        up8 = Conv2D(128, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv7))\n",
    "        merge8 = merge([conv2,up8], mode = 'concat', concat_axis = 3)\n",
    "        conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge8)\n",
    "        conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv8)\n",
    "\n",
    "        up9 = Conv2D(64, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv8))\n",
    "        merge9 = merge([conv1,up9], mode = 'concat', concat_axis = 3)\n",
    "        conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge9)\n",
    "        conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
    "        conv9 = Conv2D(2, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
    "        conv10 = Conv2D(1, 1, activation = 'sigmoid')(conv9)\n",
    "\n",
    "        inputs = Input(self.input_shape)\n",
    "        model = Model(input = inputs, output = conv10)\n",
    "                \n",
    "        model.summary()\n",
    "\n",
    "#         feature = Input(shape=(self.input_shape,), dtype='float32')\n",
    "#         select_prob = model(feature)\n",
    "\n",
    "        #return Model(feature, select_prob)\n",
    "        return model\n",
    "\n",
    "    #%% Discriminator (Critic)\n",
    "    def build_discriminator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "                \n",
    "        model.add(Dense(200, activation=self.activation, name = 'dense1', kernel_regularizer=regularizers.l2(1e-3), input_dim = self.input_shape)) \n",
    "        model.add(BatchNormalization())     # Use Batch norm for preventing overfitting\n",
    "        model.add(Dense(200, activation=self.activation, name = 'dense2', kernel_regularizer=regularizers.l2(1e-3)))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dense(2, activation ='softmax', name = 'dense3', kernel_regularizer=regularizers.l2(1e-3)))\n",
    "        \n",
    "        model.summary()\n",
    "        \n",
    "        # There are two inputs to be used in the discriminator\n",
    "        # 1. Features\n",
    "        feature = Input(shape=(self.input_shape,), dtype='float32')\n",
    "        # 2. Selected Features\n",
    "        select = Input(shape=(self.input_shape,), dtype='float32')         \n",
    "        \n",
    "        # Element-wise multiplication\n",
    "        model_input = Multiply()([feature, select])\n",
    "        prob = model(model_input)\n",
    "\n",
    "        return Model([feature, select], prob)\n",
    "\n",
    "    #%% Sampling the features based on the output of the generator\n",
    "    def Sample_M(self, gen_prob):\n",
    "        \n",
    "        # Shape of the selection probability\n",
    "        n = gen_prob.shape[0]\n",
    "        d = gen_prob.shape[1]\n",
    "                \n",
    "        # Sampling\n",
    "        samples = np.random.binomial(1, gen_prob, (n,d))\n",
    "        \n",
    "        return samples\n",
    "\n",
    "    #%% Training procedure\n",
    "    def train(self, x_train, y_train):\n",
    "\n",
    "        # For each epoch (actually iterations)\n",
    "        for epoch in range(self.epochs):\n",
    "\n",
    "            #%% Train Discriminator\n",
    "            # Select a random batch of samples\n",
    "            idx = np.random.randint(0, x_train.shape[0], self.batch_size)\n",
    "            x_batch = x_train[idx,:]\n",
    "            y_batch = y_train[idx,:]\n",
    "\n",
    "            # Generate a batch of probabilities of feature selection\n",
    "            gen_prob = self.generator.predict(x_batch)\n",
    "            \n",
    "            # Sampling the features based on the generated probability\n",
    "            sel_prob = self.Sample_M(gen_prob)     \n",
    "            \n",
    "            # Compute the prediction of the critic based on the sampled features (used for generator training)\n",
    "            dis_prob = self.discriminator.predict([x_batch, sel_prob])\n",
    "\n",
    "            # Train the discriminator\n",
    "            d_loss = self.discriminator.train_on_batch([x_batch, sel_prob], y_batch)\n",
    "\n",
    "            #%% Train Generator\n",
    "            # Use three things as the y_true: sel_prob, dis_prob, and ground truth (y_batch)\n",
    "            y_batch_final = np.concatenate( (sel_prob, np.asarray(dis_prob), y_batch), axis = 1 )\n",
    "\n",
    "            # Train the generator\n",
    "            g_loss = self.generator.train_on_batch(x_batch, y_batch_final)\n",
    "\n",
    "            #%% Plot the progress\n",
    "            dialog = 'Epoch: ' + str(epoch) + ', d_loss (CE): ' + str(np.round(d_loss[0],4)) + ', d_loss (Acc): ' + str(d_loss[1]) + ', g_loss: ' + str(np.round(g_loss,4))\n",
    " \n",
    "            if epoch % 100 == 0:              \n",
    "                print(dialog)\n",
    "    \n",
    "    #%% Selected Features        \n",
    "    def output(self, x_train):\n",
    "        \n",
    "        gen_prob = self.generator.predict(x_train)\n",
    "        \n",
    "        return np.asarray(gen_prob)\n",
    "\n",
    "\n",
    "#%% Main Function\n",
    "if __name__ == '__main__':\n",
    "        \n",
    "    # Data generation function import\n",
    "    from Data_Generation import generate_data\n",
    "    \n",
    "    #%% Parameters\n",
    "    # Synthetic data type    \n",
    "    idx = 5\n",
    "    data_sets = ['Syn1','Syn2','Syn3','Syn4','Syn5','Syn6']\n",
    "    data_type = data_sets[idx]\n",
    "    \n",
    "    # No need to provide the number of relevant features at all!\n",
    "\n",
    "    # Data output can be either binary (Y) or Probability (Prob)\n",
    "    data_out_sets = ['Y','Prob']\n",
    "    data_out = data_out_sets[0]\n",
    "    \n",
    "    # Number of Training and Testing samples\n",
    "    train_N = 10000\n",
    "    test_N = 10000\n",
    "    \n",
    "    # Seeds (different seeds for training and testing)\n",
    "    train_seed = 0\n",
    "    test_seed = 1\n",
    "        \n",
    "    #%% Data Generation (Train/Test)\n",
    "    def create_data(data_type, data_out): \n",
    "        \n",
    "        x_train, y_train, g_train = generate_data(n = train_N, data_type = data_type, seed = train_seed, out = data_out)  \n",
    "        x_test,  y_test,  g_test  = generate_data(n = test_N,  data_type = data_type, seed = test_seed,  out = data_out)  \n",
    "    \n",
    "        return x_train, y_train, g_train, x_test, y_test, g_test\n",
    "    \n",
    "    x_train, y_train, g_train, x_test, y_test, g_test = create_data(data_type, data_out)\n",
    "\n",
    "    #%% Hyperparameter\n",
    "    lamda = 3\n",
    "\n",
    "    # 1. PVS Class call\n",
    "    PVS_Alg = PVS(x_train, data_type, lamda)\n",
    "        \n",
    "    # 2. Algorithm training\n",
    "    PVS_Alg.train(x_train, y_train)    \n",
    "    \n",
    "    # 3. Get the selection probability on the testing set\n",
    "    Sel_Prob_Test = PVS_Alg.output(x_test)\n",
    "    \n",
    "    # 4. Selected features\n",
    "    score = 1.*(Sel_Prob_Test > 0.5)\n",
    "    \n",
    "    #%% Performance Metrics\n",
    "    def performance_metric(score, g_truth):\n",
    "\n",
    "        n = len(score)\n",
    "        Temp_TPR = np.zeros([n,])\n",
    "        Temp_FDR = np.zeros([n,])\n",
    "        \n",
    "        for i in range(n):\n",
    "    \n",
    "            # TPR    \n",
    "            TPR_Nom = np.sum(score[i,:] * g_truth[i,:])\n",
    "            TPR_Den = np.sum(g_truth[i,:])\n",
    "            Temp_TPR[i] = 100 * float(TPR_Nom)/float(TPR_Den+1e-8)\n",
    "        \n",
    "            # FDR\n",
    "            FDR_Nom = np.sum(score[i,:] * (1-g_truth[i,:]))\n",
    "            FDR_Den = np.sum(score[i,:])\n",
    "            Temp_FDR[i] = 100 * float(FDR_Nom)/float(FDR_Den+1e-8)\n",
    "    \n",
    "        return np.mean(Temp_TPR), np.mean(Temp_FDR), np.std(Temp_TPR), np.std(Temp_FDR)\n",
    "    \n",
    "    #%% Output\n",
    "        \n",
    "    TPR_mean, FDR_mean, TPR_std, FDR_std = performance_metric(score, g_test)\n",
    "        \n",
    "    print('TPR mean: ' + str(np.round(TPR_mean,1)) + '\\%, ' + 'TPR std: ' + str(np.round(TPR_std,1)) + '\\%, '  )\n",
    "    print('FDR mean: ' + str(np.round(FDR_mean,1)) + '\\%, ' + 'FDR std: ' + str(np.round(FDR_std,1)) + '\\%, '  )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
